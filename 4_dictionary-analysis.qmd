---
title: "Session 4: Dictionary Analysis"
author: "Wouter van Atteveldt & Johannes B. Gruber"
subtitle: "Textanalyse in R: eine Einf√ºhrung"
date: "2023-03-27"
format:
  html:
    toc: true
    toc-location: left
bibliography: references.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE
)
```

# Introduction

Dictionaries are a very transparent and useful tool for automatic content analysis.
At its simplest, a dictionary is a list of terms, of lexicons, with a specific meaning attached to each term. 
For example, a sentiment lexicon can contain a list of positive and negative words. 
The computer then counts the total number of negative and positive words per document, 
giving an indication of the sentiment of the document. 

This can be expanded by also using wildcards, boolean, phrase and proximity conditions:
wildcards such as `immig*` would match all words starting with or containing a certain term;
boolean conditions allow you to specify that specific combinations of words must occur;
while phrase and proximity conditions specify that words need to occur next to or near each other. 

Whatever type of dictionary is used, it is vital that the dictionary is validated in the context of its use:
does the occurrence of the specified terms indeed imply that the desired theoretical concept is present?
The most common approach to validation is *gold standard* validation:
human expert coding is used to code a subset of documents, and the computer output is validated against the (presumed) correct coding.

# Lexical Sentiment Analysis with `tidytext`

The easiest setup for dictionary analysis is finding exact matches with an existing word list or lexicon.
For example, there are various sentiment lexicons that assign a positive or negative label to words. 

For example, the [textdata](https://cran.r-project.org/web/packages/textdata/textdata.pdf) package
contains a number of lexica, including the NRC emotion lexicon:

```{r}
library(textdata)
nrc <- lexicon_nrc()
head(nrc)
```

Using the various `join` functions, it is easy to match this lexicon to a token list. 
For example, let's see which emotional terms occur in the state of the union speeches:

Note: For more information on basic tidytext usage, see [our tidytext tutorial](https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/tidytext.md) and/or the [official tidytext book](https://www.tidytextmining.com/). 

```{r}
library(sotu)
library(tidyverse)
library(tidytext)
sotu_tidy <- sotu_meta  |> 
  bind_cols(text = sotu_text) |>  
  rename(doc_id = X) |> 
  unnest_tokens(word, text)
head(sotu_tidy)
```

Since both the `nrc` and `sotu_tidy` data frames contain the word column, 
we can directly join them and e.g. compute the total emotion per year:

```{r}
sotu_emotions <- left_join(sotu_tidy, nrc, by = "word")
sotu_emotions_count <- sotu_emotions|>
  count(year, sentiment) |>
  mutate(p = n / sum(n)) |>
  filter(!is.na(sentiment))
head(sotu_emotions)
```

Note the use of `left_join` to preserve unmatched tokens, which we can then use
to compute the percentage of words `p` that matched the lexicon. 

So, how did emotions change over time?

```{r}
library(ggridges)
ggplot(sotu_emotions_count) +
  geom_ridgeline(aes(x = year, y = sentiment, height = p / max(p), fill = sentiment)) +
  theme_ridges() +
  guides(fill = "none")
```

# Inspecting dictionary hits

Using the `tokenbrowser` package developed by Kasper Welbers,
we can inspect the hits in their original context.

(Note that due to an unfortunate bug, this package requires the document id column is called `doc_id`)

```{r}
#| eval: false
library(tokenbrowser)
hits <- left_join(sotu_tidy, nrc)
meta <- select(sotu_tidy, doc_id, year, president, party)
categorical_browser(hits, meta = meta, category = hits$sentiment, token_col = "word") |>
  browseURL()
```

Note also that some words are repeated since the join will duplicate the rows if a word matched multiple categories.

# Most common words per category

```{r}
sotu_emotions |> 
  count(word, sentiment) |> 
  group_by(sentiment) |> 
  slice_max(n, n = 6) |> 
  filter(!is.na(sentiment)) |> 
  mutate(word = reorder_within(word, n, sentiment)) %>%
  ggplot(aes(x = n, y = word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) +
  scale_y_reordered()
```

# Smarter Wordclouds

One easy visualation for text that you will often see is a word cloud:

```{r}
library(wordcloud)
sotu_emotions |> 
  count(word) |> 
  with(wordcloud(word, n, max.words = 200, colors = brewer.pal(9, "Pastel1")))
```

While common, however, it is not really the best way to visualise anything:

- the location of words on the x and y axis is random, which might look nice, but wastes potential to map more aesthetics
- the colour and size of words communicate the same thing: prevalence
- mapping size to prevalence makes the plot hard to interpret beyond the largest words

In the end, this table tells you more about the data than the visualisation:

```{r}
library(gt)
sotu_emotions |> 
  count(word, sentiment, sort = TRUE) |> 
  head(200)|> 
  gt() |> 
  data_color(
    columns = sentiment,
    colors = scales::col_factor(
      palette = "viridis", 
      domain = unique(nrc$sentiment)
    )
  ) |> 
  tab_options(
    container.overflow.y = TRUE,
    container.height = "400px"
  )
```

And if you wanted a plot, you could communicate more details with a simple bar plot.

```{r}
sotu_emotions |> 
  filter(!is.na(sentiment)) |> 
  count(word, sentiment, sort = TRUE) |> 
  mutate(word = reorder(word, n)) %>%
  head(20) |> 
  ggplot(aes(x = n, y = word, fill = sentiment)) +
  geom_col(position = "dodge")
```

Now to a smarter version of the word cloud that actually maps several variables to aesthetics.
I calculate the keyness of words using the weighted log odds.
High values mean words are more common in a dataset than in a comparison set.

```{r}
library(tidylo)
sotu_emotions_keyness <- sotu_emotions |> 
  filter(year > 1989) |> # narrow to Clinton and after
  count(party, word, sentiment) |> 
  bind_log_odds(party, word, n)
head(sotu_emotions_keyness)
```

Now we have several variables that we can map to aesthetics using a `ggplot2` addon called `ggwordcloud`:

```{r}
library("ggwordcloud")
sotu_emotions_keyness %>%
  filter(!is.na(sentiment)) |> 
  slice_max(log_odds_weighted, n = 200) |> 
  # reverse the scale for Democrats to show their log odds as negative values
  mutate(overrepresentation = ifelse(party == "Democratic", 
                                     log_odds_weighted * -1, 
                                     log_odds_weighted)) |> 
  ggplot(aes(
    x = overrepresentation,
    label = word, size = n, colour = sentiment
  )) +
  geom_text_wordcloud(show.legend = TRUE)  +
  scale_x_continuous(labels = function(x) abs(x)) +
  scale_size_area(max_size = 7, guide = "none") +
  theme_minimal() +
  labs(title = "Democratic (left) vs Republican (right) overrepresentation of words in SOTU since 1993")
```

Here:

- the x axis shows over-representation for words in either Democratic or Republican speeches
- (the y axis is left random, but could be used to show another variable)
- the colour shows the sentiment of a word
- the size still shows the prevalence

# More complicated dictionaries

For more complicated dictionaries, you can use the boolydict package. 
At the time of writing, this package needs to be installed from github rather than from CRAN:

(Note: This might need rtools to build, hopefully it will work on non-linux computers!)

```{r}
remotes::install_github('kasperwelbers/boolydict')
```

Now, we can create a dictionary containing e.g. boolean and wildcard terms.
For example, we can create a (very naive) dictionary for Islamic terrorism and immigration from Islamic countries:

```{r}
library(boolydict)
dictionary = tribble(
  ~label, ~string,
  'islam_terror', '(musl* OR islam*) AND terror*',
  'islam_immig', '(musl* OR islam*) AND immig*',
)
```

Now, we can use the `dict_add` function to add a column for each dictionary label,
using `by_label` to create separate columns, and settings `fill=0` for words that did not match:

```{r}
hits <- sotu_tidy |>
  dict_add(dictionary, text_col = "word", context_col = "doc_id", by_label = "label", fill = 0) |>
  as_tibble()
hits |>
  arrange(-islam_immig) |>
  head()
```

So, how did mentions of Islam-related terrorism and immigration change over time?

```{r}
hits |>
  select(year, islam_immig, islam_terror) |>
  pivot_longer(-year) |>
  group_by(year, name) |>
  summarize(value = sum(value)) |>
  ggplot() +
  geom_line(aes(x = year, y = value, color = name), alpha = .6)
```

Unsurprisingly, both concepts only really became salient after 2000.

